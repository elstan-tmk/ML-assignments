{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCZYXwtCsL_y"
   },
   "source": [
    "CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm. \n",
    "\n",
    "This assignment contains:\n",
    "1. The codes are correctly working with the given parts of the program.\n",
    "2. Codes clearly explained through \"Markdown\" texts and commenting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p_DvtT7sOIr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Import all necessary libraries.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of words from a collection of email stored in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjKF0nIMwz8_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First create a defination for the root_dir\n",
    "# use os.path.join to join path components into a single complete file path\n",
    "# for every mail in the folder, split the words into a dictionary\n",
    "\n",
    "# list_to_remove is created by taking all the keys of the dictionary\n",
    "\n",
    "def make_Dictionary(root_dir):\n",
    "  all_words = []\n",
    "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)] \n",
    "  for mail in emails:\n",
    "    with open(mail) as m:\n",
    "      for line in m:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "  dictionary = Counter(all_words)\n",
    "  list_to_remove = list(dictionary)\n",
    "\n",
    "# Loop through each item in list_to_remove and check if it is an alphabetic string using item.isalpha()\n",
    "# Check if the length of the word is more than 1 character\n",
    "# if either of the conditions is false, the item will be deleted from the dictionary \n",
    "  for item in list_to_remove:\n",
    "    if item.isalpha() == False:\n",
    "      del dictionary[item]\n",
    "    elif len(item) == 1:\n",
    "      del dictionary[item]\n",
    "  dictionary = dictionary.most_common(3000)\n",
    "  return dictionary\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another extract function that takes in a directory that contains email files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmVW5xNlyOFc"
   },
   "outputs": [],
   "source": [
    "# Create a list of file paths by joining the file names in \"mail_dir\" with the directory path.\n",
    "# Then initializes arrays of size length of the files times the 3000.\n",
    "\n",
    "# the function counts the occurrences of each word in the dictionary in each email file\n",
    "# and stores the count in the corresponding position in the \"features_matrix\"\n",
    "\n",
    "# id the file name starts with \"spmsg\", the corresponding label in \"train_labels\" is set to 1\n",
    "\n",
    "# this function returns the extracted feature matrix and labels.\n",
    "\n",
    "def extract_features(mail_dir):\n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "  train_labels = np.zeros(len(files))\n",
    "  count = 1;\n",
    "  docID = 0;\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "      for i, line in enumerate(fi):\n",
    "        if i ==2:\n",
    "          words = line.split()\n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            for i, d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "      train_labels[docID] = 0;\n",
    "      filepathTokens = fil.split('/')\n",
    "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "        train_labels[docID] = 1;\n",
    "        count = count + 1\n",
    "      docID = docID + 1\n",
    "  return features_matrix, train_labels                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoq-rE7Mx0pp"
   },
   "outputs": [],
   "source": [
    "# Locate the Data path\n",
    "\n",
    "TRAIN_DIR = \"Data/train-mails\"\n",
    "TEST_DIR =\"Data/test-mails\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Prepare the data for modeling: the data has been splited into training and test sets, and so we use the CountVectorizer to convert the email text into a numerical representation that can be used by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 127480,
     "status": "ok",
     "timestamp": 1578886833446,
     "user": {
      "displayName": "Arin Brahma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBXGIW7FvUnbm_QmEFGh4rLebuLHNZgc8PuNinU=s64",
      "userId": "05299564422021375910"
     },
     "user_tz": 480
    },
    "id": "134lmhauyQxE",
    "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n"
     ]
    }
   ],
   "source": [
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "\n",
    "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_features_matrix, test_labels = extract_features(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifier: scikit-learn library is used to train a Multinomial Naive Bayes classifier on the training data with GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model using Gaussian Naive Bayes algoritm....\n",
      "Training completed\n",
      "testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data ... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
      "Accuracy: 0.9653846153846154\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "print(\"Training Model using Gaussian Naive Bayes algoritm....\")\n",
    "clf.fit(features_matrix, labels)\n",
    "print(\"Training completed\")\n",
    "print(\"testing trained model to predict Test Data labels\")\n",
    "y_pred = clf.predict(test_features_matrix)\n",
    "print(\"Completed classification of the Test Data ... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classifier: You can use the trained classifier to make predictions on the test data, and then evaluate the accuracy of the classifier using the accuracy_score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5_mPrvN586A"
   },
   "source": [
    "======================= END OF PROGRAM ========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaSi3qlFUlqTup/1esXCKD",
   "collapsed_sections": [],
   "name": "naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf3153f29b6a954d62afc0116bc41f9efdca119a3bb29d09d72b104f272df27d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
